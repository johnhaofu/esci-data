# GRPO (Group Relative Policy Optimization) 算法复现

## 项目简介

本项目复现了GRPO（群组相对策略优化）算法，这是一种用于训练大语言模型的强化学习算法，最初在DeepSeekMath论文中提出。GRPO被成功应用于DeepSeek R1模型的训练，在数学推理任务上取得了显著成果。

## 算法核心思想

GRPO的核心创新在于**使用群组内奖励均值作为基线来计算优势**，从而避免了训练价值函数的需要。这种方法具有以下优点：

- **内存效率**：不需要训练单独的价值函数网络
- **计算简单**：群组相对优势计算直观且高效
- **训练稳定**：裁剪机制确保策略更新的稳定性
- **效果显著**：在数学推理等任务上表现优异

## 算法步骤

1. **生成群组响应**：对每个prompt生成K个不同的响应
2. **计算奖励**：使用奖励模型评估每个响应的质量
3. **计算群组基线**：R̄ = (1/K) Σ R_i，使用群组平均奖励作为基线
4. **计算相对优势**：A_i = R_i - R̄，每个响应相对于群组的优势
5. **策略更新**：使用裁剪目标函数更新策略参数

## 数学公式

GRPO的目标函数为：

```
L = E[min(r(θ)·A, clip(r(θ), 1-ε, 1+ε)·A)] - β·KL(π_θ || π_old)
```

其中：
- `r(θ) = π_θ(a|s) / π_old(a|s)` 是重要性比率
- `A = R - R̄` 是群组相对优势
- `R̄ = (1/K) Σ R_i` 是群组平均奖励
- `ε` 是裁剪参数，防止过大的策略更新
- `β` 是KL散度系数

## 项目结构

```
.
├── grpo_algorithm.py    # 完整的GRPO算法实现
├── grpo_demo.py        # 简化的演示版本
├── README.md           # 项目说明文档
└── requirements.txt    # 依赖包列表
```

## 文件说明

### `grpo_algorithm.py`
完整的GRPO算法实现，包含：
- `GRPOConfig`: 算法配置参数
- `GRPOTrainer`: 完整的训练器实现
- 支持实际的神经网络模型训练

### `grpo_demo.py`
简化的演示版本，使用模拟数据展示算法核心概念：
- 不依赖复杂的神经网络模型
- 清晰展示算法的每个步骤
- 包含详细的算法原理解释

## 快速开始

### 1. 安装依赖

```bash
pip install numpy matplotlib torch
```

### 2. 运行演示

```bash
python grpo_demo.py
```

演示程序将展示：
- 算法原理详解
- 单步训练过程演示
- 多步训练模拟
- 训练进度可视化

### 3. 查看完整实现

```bash
python grpo_algorithm.py
```

## 演示输出示例

```
🚀 GRPO (Group Relative Policy Optimization) 算法复现
   基于DeepSeekMath论文的强化学习算法

================================================================================
GRPO (Group Relative Policy Optimization) 算法原理详解
================================================================================

🎯 核心思想:
   GRPO通过群组内的相对比较来估计优势，避免了训练价值函数的需要

📊 算法步骤:
   1. 生成群组响应：对每个prompt生成K个不同的响应
   2. 计算奖励：使用奖励模型评估每个响应的质量
   3. 计算群组基线：R̄ = (1/K) Σ R_i，使用群组平均奖励作为基线
   4. 计算相对优势：A_i = R_i - R̄，每个响应相对于群组的优势
   5. 策略更新：使用裁剪目标函数更新策略参数

============================================================
GRPO训练步骤演示 - Prompt: 2 + 3
============================================================

1. 生成响应群组:
   响应1: 解答：2 + 3的答案是5
   响应2: 计算：2 + 3 = 5
   响应3: 让我算一下：2 + 3等于5
   响应4: 简单计算：2 + 3的结果是5
   响应5: 数学计算：2 + 3 = 6
   响应6: 我觉得2 + 3等于4
   响应7: 根据计算：2 + 3 = 5
   响应8: 答案：2 + 3是5

2. 计算奖励:
   响应1: 1.052
   响应2: 0.943
   响应3: 1.089
   响应4: 0.967
   响应5: -0.456
   响应6: -0.289
   响应7: 1.012
   响应8: 0.998

3. 计算群组相对优势:
   群组平均奖励: 0.665
   各响应优势:
     响应1: +0.387 (正优势)
     响应2: +0.278 (正优势)
     响应3: +0.424 (正优势)
     响应4: +0.302 (正优势)
     响应5: -1.121 (负优势)
     响应6: -0.954 (负优势)
     响应7: +0.347 (正优势)
     响应8: +0.333 (正优势)
```

## 与PPO的对比

| 特性 | PPO | GRPO |
|------|-----|------|
| 优势估计 | 需要价值函数 V(s) | 使用群组均值 R̄ |
| 内存使用 | 高（需要价值网络） | 低（无需价值网络） |
| 计算复杂度 | 高 | 低 |
| 训练稳定性 | 稳定 | 稳定 |
| 适用场景 | 通用RL任务 | 文本生成任务 |

## 应用场景

GRPO特别适用于以下任务：
- 数学问题求解
- 代码生成
- 逻辑推理
- 需要精确答案验证的任务

## 参考文献

1. **DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models** - 原始GRPO论文
2. **DeepSeek R1: Incentivizing reasoning capability in llms via reinforcement learning** - GRPO在实际模型中的应用
3. **Proximal Policy Optimization Algorithms** - PPO算法原理

## 技术特点

### 核心优势
- **内存效率**：相比PPO减少约50%的GPU内存使用
- **实现简单**：算法逻辑清晰，易于理解和实现
- **效果显著**：在GSM8K数学推理任务上显著提升性能

### 算法创新
- **群组基线**：使用群组内平均奖励作为基线，避免价值函数训练
- **相对优势**：通过群组内比较计算优势，减少方差
- **裁剪机制**：继承PPO的稳定性保证

## 贡献

欢迎提交Issue和Pull Request来改进这个项目！

## 许可证

MIT License

---

**注意**：这是GRPO算法的教学和研究用途复现，如需在生产环境中使用，请参考官方实现和相关论文。
