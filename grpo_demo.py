"""
GRPO (Group Relative Policy Optimization) ç®—æ³•æ¼”ç¤º
ç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®å±•ç¤ºç®—æ³•æ ¸å¿ƒæ€æƒ³
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import List, Dict, Tuple
import random


class SimpleGRPODemo:
    """
    ç®€åŒ–çš„GRPOç®—æ³•æ¼”ç¤º
    ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®å±•ç¤ºæ ¸å¿ƒæ¦‚å¿µ
    """
    
    def __init__(self, group_size: int = 8, epsilon: float = 0.2):
        self.group_size = group_size
        self.epsilon = epsilon
        self.training_history = []
        
    def simulate_response_generation(self, prompt: str) -> List[str]:
        """
        æ¨¡æ‹Ÿä¸ºpromptç”Ÿæˆå¤šä¸ªå“åº”
        """
        base_responses = [
            f"è§£ç­”ï¼š{prompt}çš„ç­”æ¡ˆæ˜¯5",
            f"è®¡ç®—ï¼š{prompt} = 5",
            f"è®©æˆ‘ç®—ä¸€ä¸‹ï¼š{prompt}ç­‰äº5",
            f"ç®€å•è®¡ç®—ï¼š{prompt}çš„ç»“æœæ˜¯5",
            f"æ•°å­¦è®¡ç®—ï¼š{prompt} = 6",  # é”™è¯¯ç­”æ¡ˆ
            f"æˆ‘è§‰å¾—{prompt}ç­‰äº4",    # é”™è¯¯ç­”æ¡ˆ
            f"æ ¹æ®è®¡ç®—ï¼š{prompt} = 5",
            f"ç­”æ¡ˆï¼š{prompt}æ˜¯5"
        ]
        
        # éšæœºé€‰æ‹©group_sizeä¸ªå“åº”
        return random.sample(base_responses, self.group_size)
    
    def compute_rewards(self, prompt: str, responses: List[str]) -> List[float]:
        """
        æ¨¡æ‹Ÿå¥–åŠ±è®¡ç®—
        æ­£ç¡®ç­”æ¡ˆï¼ˆåŒ…å«"5"ï¼‰è·å¾—é«˜å¥–åŠ±ï¼Œé”™è¯¯ç­”æ¡ˆè·å¾—ä½å¥–åŠ±
        """
        rewards = []
        
        for response in responses:
            if "= 5" in response or "æ˜¯5" in response or "ç­‰äº5" in response:
                # æ­£ç¡®ç­”æ¡ˆï¼Œæ·»åŠ ä¸€äº›éšæœºæ€§
                reward = np.random.normal(1.0, 0.1)
            elif "= 6" in response or "æ˜¯6" in response or "ç­‰äº6" in response:
                # é”™è¯¯ç­”æ¡ˆ
                reward = np.random.normal(-0.5, 0.1)
            elif "= 4" in response or "æ˜¯4" in response or "ç­‰äº4" in response:
                # é”™è¯¯ç­”æ¡ˆ
                reward = np.random.normal(-0.3, 0.1)
            else:
                # å…¶ä»–æƒ…å†µ
                reward = np.random.normal(0.0, 0.2)
            
            rewards.append(reward)
        
        return rewards
    
    def compute_group_advantages(self, rewards: List[float]) -> Tuple[float, List[float]]:
        """
        è®¡ç®—ç¾¤ç»„ç›¸å¯¹ä¼˜åŠ¿
        
        GRPOæ ¸å¿ƒï¼šä¼˜åŠ¿ = å¥–åŠ± - ç¾¤ç»„å¹³å‡å¥–åŠ±
        
        Returns:
            ç¾¤ç»„å¹³å‡å¥–åŠ±, ä¼˜åŠ¿åˆ—è¡¨
        """
        mean_reward = np.mean(rewards)
        advantages = [reward - mean_reward for reward in rewards]
        return mean_reward, advantages
    
    def compute_importance_ratios(self, advantages: List[float]) -> List[float]:
        """
        æ¨¡æ‹Ÿé‡è¦æ€§æ¯”ç‡è®¡ç®—
        åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™æ˜¯æ–°ç­–ç•¥æ¦‚ç‡/æ—§ç­–ç•¥æ¦‚ç‡
        è¿™é‡Œç”¨ç®€åŒ–çš„æ¨¡æ‹Ÿ
        """
        # æ¨¡æ‹Ÿé‡è¦æ€§æ¯”ç‡ï¼ŒåŸºäºä¼˜åŠ¿å¤§å°
        ratios = []
        for advantage in advantages:
            # æ­£ä¼˜åŠ¿çš„å“åº”æœ‰æ›´é«˜çš„æ¯”ç‡
            if advantage > 0:
                ratio = np.random.uniform(1.0, 1.5)
            else:
                ratio = np.random.uniform(0.7, 1.0)
            ratios.append(ratio)
        
        return ratios
    
    def compute_clipped_objective(
        self, 
        ratios: List[float], 
        advantages: List[float]
    ) -> List[float]:
        """
        è®¡ç®—GRPOçš„è£å‰ªç›®æ ‡å‡½æ•°
        
        ç›®æ ‡ = min(ratio * advantage, clip(ratio, 1-Îµ, 1+Îµ) * advantage)
        """
        objectives = []
        
        for ratio, advantage in zip(ratios, advantages):
            # è£å‰ªé‡è¦æ€§æ¯”ç‡
            clipped_ratio = np.clip(ratio, 1 - self.epsilon, 1 + self.epsilon)
            
            # è®¡ç®—ä¸¤ä¸ªç›®æ ‡å€¼å¹¶å–æœ€å°å€¼
            obj1 = ratio * advantage
            obj2 = clipped_ratio * advantage
            
            objective = min(obj1, obj2)
            objectives.append(objective)
        
        return objectives
    
    def demonstrate_single_step(self, prompt: str) -> Dict:
        """
        æ¼”ç¤ºå•æ­¥GRPOè®­ç»ƒè¿‡ç¨‹
        """
        print(f"\n{'='*60}")
        print(f"GRPOè®­ç»ƒæ­¥éª¤æ¼”ç¤º - Prompt: {prompt}")
        print(f"{'='*60}")
        
        # 1. ç”Ÿæˆå“åº”ç¾¤ç»„
        print("\n1. ç”Ÿæˆå“åº”ç¾¤ç»„:")
        responses = self.simulate_response_generation(prompt)
        for i, response in enumerate(responses):
            print(f"   å“åº”{i+1}: {response}")
        
        # 2. è®¡ç®—å¥–åŠ±
        print("\n2. è®¡ç®—å¥–åŠ±:")
        rewards = self.compute_rewards(prompt, responses)
        for i, (response, reward) in enumerate(zip(responses, rewards)):
            print(f"   å“åº”{i+1}: {reward:.3f}")
        
        # 3. è®¡ç®—ç¾¤ç»„ç›¸å¯¹ä¼˜åŠ¿
        print("\n3. è®¡ç®—ç¾¤ç»„ç›¸å¯¹ä¼˜åŠ¿:")
        mean_reward, advantages = self.compute_group_advantages(rewards)
        print(f"   ç¾¤ç»„å¹³å‡å¥–åŠ±: {mean_reward:.3f}")
        print("   å„å“åº”ä¼˜åŠ¿:")
        for i, advantage in enumerate(advantages):
            status = "æ­£ä¼˜åŠ¿" if advantage > 0 else "è´Ÿä¼˜åŠ¿"
            print(f"     å“åº”{i+1}: {advantage:+.3f} ({status})")
        
        # 4. è®¡ç®—é‡è¦æ€§æ¯”ç‡
        print("\n4. è®¡ç®—é‡è¦æ€§æ¯”ç‡:")
        ratios = self.compute_importance_ratios(advantages)
        for i, ratio in enumerate(ratios):
            print(f"   å“åº”{i+1}: {ratio:.3f}")
        
        # 5. è®¡ç®—è£å‰ªç›®æ ‡
        print("\n5. è®¡ç®—GRPOè£å‰ªç›®æ ‡:")
        objectives = self.compute_clipped_objective(ratios, advantages)
        total_objective = np.mean(objectives)
        
        for i, (ratio, advantage, objective) in enumerate(zip(ratios, advantages, objectives)):
            clipped_ratio = np.clip(ratio, 1 - self.epsilon, 1 + self.epsilon)
            print(f"   å“åº”{i+1}: ratio={ratio:.3f}, clipped={clipped_ratio:.3f}, "
                  f"advantage={advantage:+.3f}, objective={objective:+.3f}")
        
        print(f"\n   å¹³å‡ç›®æ ‡å€¼: {total_objective:.3f}")
        
        # è®°å½•è®­ç»ƒå†å²
        step_info = {
            'mean_reward': mean_reward,
            'total_objective': total_objective,
            'positive_advantages': sum(1 for a in advantages if a > 0),
            'negative_advantages': sum(1 for a in advantages if a < 0)
        }
        
        return step_info
    
    def run_training_simulation(self, num_steps: int = 10):
        """
        è¿è¡Œå¤šæ­¥è®­ç»ƒæ¨¡æ‹Ÿ
        """
        print(f"\n{'='*80}")
        print("GRPOè®­ç»ƒæ¨¡æ‹Ÿ - å¤šæ­¥è®­ç»ƒè¿‡ç¨‹")
        print(f"{'='*80}")
        
        prompts = [
            "2 + 3",
            "1 + 4", 
            "3 + 2",
            "4 + 1",
            "0 + 5"
        ]
        
        training_stats = []
        
        for step in range(num_steps):
            # éšæœºé€‰æ‹©ä¸€ä¸ªprompt
            prompt = random.choice(prompts)
            
            print(f"\nç¬¬{step+1}æ­¥è®­ç»ƒ:")
            print(f"Prompt: {prompt}")
            
            # ç®€åŒ–çš„å•æ­¥æ¼”ç¤º
            responses = self.simulate_response_generation(prompt)
            rewards = self.compute_rewards(prompt, responses)
            mean_reward, advantages = self.compute_group_advantages(rewards)
            
            # ç»Ÿè®¡ä¿¡æ¯
            pos_advantages = sum(1 for a in advantages if a > 0)
            neg_advantages = sum(1 for a in advantages if a < 0)
            
            print(f"  å¹³å‡å¥–åŠ±: {mean_reward:.3f}")
            print(f"  æ­£ä¼˜åŠ¿å“åº”: {pos_advantages}/{self.group_size}")
            print(f"  è´Ÿä¼˜åŠ¿å“åº”: {neg_advantages}/{self.group_size}")
            
            training_stats.append({
                'step': step + 1,
                'mean_reward': mean_reward,
                'positive_ratio': pos_advantages / self.group_size
            })
        
        self.training_history = training_stats
        return training_stats
    
    def plot_training_progress(self):
        """
        ç»˜åˆ¶è®­ç»ƒè¿›åº¦
        """
        if not self.training_history:
            print("æ²¡æœ‰è®­ç»ƒå†å²æ•°æ®å¯ç»˜åˆ¶")
            return
        
        steps = [stat['step'] for stat in self.training_history]
        mean_rewards = [stat['mean_reward'] for stat in self.training_history]
        positive_ratios = [stat['positive_ratio'] for stat in self.training_history]
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))
        
        # å¹³å‡å¥–åŠ±è¶‹åŠ¿
        ax1.plot(steps, mean_rewards, 'b-o', linewidth=2, markersize=6)
        ax1.set_title('GRPOè®­ç»ƒè¿‡ç¨‹ - å¹³å‡å¥–åŠ±å˜åŒ–', fontsize=14, fontweight='bold')
        ax1.set_xlabel('è®­ç»ƒæ­¥æ•°')
        ax1.set_ylabel('å¹³å‡å¥–åŠ±')
        ax1.grid(True, alpha=0.3)
        ax1.axhline(y=0, color='r', linestyle='--', alpha=0.5)
        
        # æ­£ä¼˜åŠ¿æ¯”ä¾‹è¶‹åŠ¿
        ax2.plot(steps, positive_ratios, 'g-s', linewidth=2, markersize=6)
        ax2.set_title('GRPOè®­ç»ƒè¿‡ç¨‹ - æ­£ä¼˜åŠ¿å“åº”æ¯”ä¾‹', fontsize=14, fontweight='bold')
        ax2.set_xlabel('è®­ç»ƒæ­¥æ•°')
        ax2.set_ylabel('æ­£ä¼˜åŠ¿å“åº”æ¯”ä¾‹')
        ax2.set_ylim(0, 1)
        ax2.grid(True, alpha=0.3)
        ax2.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='50%åŸºçº¿')
        ax2.legend()
        
        plt.tight_layout()
        plt.show()
    
    def explain_algorithm(self):
        """
        è§£é‡ŠGRPOç®—æ³•åŸç†
        """
        print("\n" + "="*80)
        print("GRPO (Group Relative Policy Optimization) ç®—æ³•åŸç†è¯¦è§£")
        print("="*80)
        
        print("\nğŸ¯ æ ¸å¿ƒæ€æƒ³:")
        print("   GRPOé€šè¿‡ç¾¤ç»„å†…çš„ç›¸å¯¹æ¯”è¾ƒæ¥ä¼°è®¡ä¼˜åŠ¿ï¼Œé¿å…äº†è®­ç»ƒä»·å€¼å‡½æ•°çš„éœ€è¦")
        
        print("\nğŸ“Š ç®—æ³•æ­¥éª¤:")
        steps = [
            "ç”Ÿæˆç¾¤ç»„å“åº”ï¼šå¯¹æ¯ä¸ªpromptç”ŸæˆKä¸ªä¸åŒçš„å“åº”",
            "è®¡ç®—å¥–åŠ±ï¼šä½¿ç”¨å¥–åŠ±æ¨¡å‹è¯„ä¼°æ¯ä¸ªå“åº”çš„è´¨é‡",
            "è®¡ç®—ç¾¤ç»„åŸºçº¿ï¼šRÌ„ = (1/K) Î£ R_iï¼Œä½¿ç”¨ç¾¤ç»„å¹³å‡å¥–åŠ±ä½œä¸ºåŸºçº¿",
            "è®¡ç®—ç›¸å¯¹ä¼˜åŠ¿ï¼šA_i = R_i - RÌ„ï¼Œæ¯ä¸ªå“åº”ç›¸å¯¹äºç¾¤ç»„çš„ä¼˜åŠ¿",
            "ç­–ç•¥æ›´æ–°ï¼šä½¿ç”¨è£å‰ªç›®æ ‡å‡½æ•°æ›´æ–°ç­–ç•¥å‚æ•°"
        ]
        
        for i, step in enumerate(steps, 1):
            print(f"   {i}. {step}")
        
        print("\nğŸ§® æ•°å­¦å…¬å¼:")
        print("   ç›®æ ‡å‡½æ•°ï¼šL = E[min(r(Î¸)Â·A, clip(r(Î¸), 1-Îµ, 1+Îµ)Â·A)]")
        print("   å…¶ä¸­ï¼š")
        print("     â€¢ r(Î¸) = Ï€_Î¸(a|s) / Ï€_old(a|s) æ˜¯é‡è¦æ€§æ¯”ç‡")
        print("     â€¢ A = R - RÌ„ æ˜¯ç¾¤ç»„ç›¸å¯¹ä¼˜åŠ¿") 
        print("     â€¢ RÌ„ = (1/K) Î£ R_i æ˜¯ç¾¤ç»„å¹³å‡å¥–åŠ±")
        print("     â€¢ Îµ æ˜¯è£å‰ªå‚æ•°ï¼Œé˜²æ­¢è¿‡å¤§çš„ç­–ç•¥æ›´æ–°")
        
        print("\nâœ… ç®—æ³•ä¼˜åŠ¿:")
        advantages = [
            "å†…å­˜æ•ˆç‡ï¼šä¸éœ€è¦è®­ç»ƒå•ç‹¬çš„ä»·å€¼å‡½æ•°ç½‘ç»œ",
            "è®¡ç®—ç®€å•ï¼šç¾¤ç»„ç›¸å¯¹ä¼˜åŠ¿è®¡ç®—ç›´è§‚ä¸”é«˜æ•ˆ", 
            "è®­ç»ƒç¨³å®šï¼šè£å‰ªæœºåˆ¶ç¡®ä¿ç­–ç•¥æ›´æ–°çš„ç¨³å®šæ€§",
            "æ•ˆæœæ˜¾è‘—ï¼šåœ¨æ•°å­¦æ¨ç†ç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚"
        ]
        
        for advantage in advantages:
            print(f"   â€¢ {advantage}")
        
        print("\nğŸ¯ åº”ç”¨åœºæ™¯:")
        applications = [
            "æ•°å­¦é—®é¢˜æ±‚è§£",
            "ä»£ç ç”Ÿæˆ",
            "é€»è¾‘æ¨ç†",
            "éœ€è¦ç²¾ç¡®ç­”æ¡ˆéªŒè¯çš„ä»»åŠ¡"
        ]
        
        for app in applications:
            print(f"   â€¢ {app}")
        
        print("\nğŸ’¡ ä¸PPOçš„åŒºåˆ«:")
        print("   PPO: éœ€è¦ä»·å€¼å‡½æ•°ä¼°è®¡ä¼˜åŠ¿ â†’ A = Q(s,a) - V(s)")
        print("   GRPO: ä½¿ç”¨ç¾¤ç»„å‡å€¼ä¼°è®¡ä¼˜åŠ¿ â†’ A = R - RÌ„")
        print("   ç»“æœ: GRPOå†…å­˜ä½¿ç”¨æ›´å°‘ï¼Œè®¡ç®—æ›´ç®€å•")


def main():
    """
    ä¸»å‡½æ•°ï¼šè¿è¡ŒGRPOç®—æ³•æ¼”ç¤º
    """
    print("ğŸš€ GRPO (Group Relative Policy Optimization) ç®—æ³•å¤ç°")
    print("   åŸºäºDeepSeekMathè®ºæ–‡çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•")
    
    # åˆ›å»ºæ¼”ç¤ºå®ä¾‹
    demo = SimpleGRPODemo(group_size=8, epsilon=0.2)
    
    # 1. è§£é‡Šç®—æ³•åŸç†
    demo.explain_algorithm()
    
    # 2. å•æ­¥è¯¦ç»†æ¼”ç¤º
    demo.demonstrate_single_step("2 + 3")
    
    # 3. å¤šæ­¥è®­ç»ƒæ¨¡æ‹Ÿ
    print(f"\n{'='*80}")
    print("å¼€å§‹å¤šæ­¥è®­ç»ƒæ¨¡æ‹Ÿ...")
    training_stats = demo.run_training_simulation(num_steps=15)
    
    # 4. æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡
    print(f"\n{'='*60}")
    print("è®­ç»ƒç»Ÿè®¡æ€»ç»“:")
    print(f"{'='*60}")
    
    final_stats = training_stats[-5:]  # æœ€å5æ­¥
    avg_reward = np.mean([s['mean_reward'] for s in final_stats])
    avg_positive_ratio = np.mean([s['positive_ratio'] for s in final_stats])
    
    print(f"æœ€å5æ­¥å¹³å‡å¥–åŠ±: {avg_reward:.3f}")
    print(f"æœ€å5æ­¥æ­£ä¼˜åŠ¿æ¯”ä¾‹: {avg_positive_ratio:.3f}")
    
    if avg_positive_ratio > 0.6:
        print("âœ… è®­ç»ƒæ•ˆæœè‰¯å¥½ï¼æ­£ä¼˜åŠ¿å“åº”æ¯”ä¾‹è¾ƒé«˜")
    elif avg_positive_ratio > 0.4:
        print("âš ï¸  è®­ç»ƒæ•ˆæœä¸€èˆ¬ï¼Œéœ€è¦æ›´å¤šè¿­ä»£")
    else:
        print("âŒ è®­ç»ƒæ•ˆæœä¸ä½³ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´å‚æ•°")
    
    # 5. ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼ˆå¦‚æœæœ‰matplotlibï¼‰
    try:
        demo.plot_training_progress()
    except ImportError:
        print("\næ³¨æ„ï¼šéœ€è¦å®‰è£…matplotlibæ¥ç»˜åˆ¶è®­ç»ƒæ›²çº¿")
        print("è¿è¡Œ: pip install matplotlib")
    
    print(f"\n{'='*80}")
    print("ğŸ‰ GRPOç®—æ³•æ¼”ç¤ºå®Œæˆï¼")
    print("è¿™ä¸ªæ¼”ç¤ºå±•ç¤ºäº†GRPOçš„æ ¸å¿ƒæ€æƒ³ï¼š")
    print("1. ç¾¤ç»„ç”Ÿæˆå¤šä¸ªå“åº”")
    print("2. ä½¿ç”¨ç¾¤ç»„å‡å€¼ä½œä¸ºåŸºçº¿è®¡ç®—ç›¸å¯¹ä¼˜åŠ¿") 
    print("3. é€šè¿‡è£å‰ªç›®æ ‡å‡½æ•°ç¨³å®šè®­ç»ƒ")
    print("4. é¿å…è®­ç»ƒä»·å€¼å‡½æ•°ï¼Œæé«˜å†…å­˜æ•ˆç‡")
    print(f"{'='*80}")


if __name__ == "__main__":
    main()