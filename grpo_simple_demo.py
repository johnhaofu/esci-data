"""
GRPO (Group Relative Policy Optimization) ç®—æ³•æ¼”ç¤º
ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸ä¾èµ–å¤–éƒ¨åº“ï¼Œçº¯Pythonå®ç°
"""

import random
import math


def demonstrate_grpo_algorithm():
    """
    æ¼”ç¤ºGRPOç®—æ³•çš„æ ¸å¿ƒæ¦‚å¿µ
    """
    print("ğŸš€ GRPO (Group Relative Policy Optimization) ç®—æ³•å¤ç°")
    print("   åŸºäºDeepSeekMathè®ºæ–‡çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•")
    print("="*80)
    
    # ç®—æ³•åŸç†è§£é‡Š
    print("\nğŸ“š ç®—æ³•åŸç†è¯¦è§£:")
    print("="*60)
    
    print("\nğŸ¯ æ ¸å¿ƒæ€æƒ³:")
    print("   GRPOé€šè¿‡ç¾¤ç»„å†…çš„ç›¸å¯¹æ¯”è¾ƒæ¥ä¼°è®¡ä¼˜åŠ¿ï¼Œé¿å…äº†è®­ç»ƒä»·å€¼å‡½æ•°çš„éœ€è¦")
    print("   è¿™ç§æ–¹æ³•æ˜¾è‘—å‡å°‘äº†å†…å­˜ä½¿ç”¨ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡")
    
    print("\nğŸ“Š ç®—æ³•æ­¥éª¤:")
    steps = [
        "ç”Ÿæˆç¾¤ç»„å“åº”ï¼šå¯¹æ¯ä¸ªpromptç”ŸæˆKä¸ªä¸åŒçš„å“åº”",
        "è®¡ç®—å¥–åŠ±ï¼šä½¿ç”¨å¥–åŠ±æ¨¡å‹è¯„ä¼°æ¯ä¸ªå“åº”çš„è´¨é‡",
        "è®¡ç®—ç¾¤ç»„åŸºçº¿ï¼šRÌ„ = (1/K) Î£ R_iï¼Œä½¿ç”¨ç¾¤ç»„å¹³å‡å¥–åŠ±ä½œä¸ºåŸºçº¿",
        "è®¡ç®—ç›¸å¯¹ä¼˜åŠ¿ï¼šA_i = R_i - RÌ„ï¼Œæ¯ä¸ªå“åº”ç›¸å¯¹äºç¾¤ç»„çš„ä¼˜åŠ¿",
        "ç­–ç•¥æ›´æ–°ï¼šä½¿ç”¨è£å‰ªç›®æ ‡å‡½æ•°æ›´æ–°ç­–ç•¥å‚æ•°"
    ]
    
    for i, step in enumerate(steps, 1):
        print(f"   {i}. {step}")
    
    print("\nğŸ§® æ•°å­¦å…¬å¼:")
    print("   ç›®æ ‡å‡½æ•°ï¼šL = E[min(r(Î¸)Â·A, clip(r(Î¸), 1-Îµ, 1+Îµ)Â·A)]")
    print("   å…¶ä¸­ï¼š")
    print("     â€¢ r(Î¸) = Ï€_Î¸(a|s) / Ï€_old(a|s) æ˜¯é‡è¦æ€§æ¯”ç‡")
    print("     â€¢ A = R - RÌ„ æ˜¯ç¾¤ç»„ç›¸å¯¹ä¼˜åŠ¿") 
    print("     â€¢ RÌ„ = (1/K) Î£ R_i æ˜¯ç¾¤ç»„å¹³å‡å¥–åŠ±")
    print("     â€¢ Îµ æ˜¯è£å‰ªå‚æ•°ï¼Œé˜²æ­¢è¿‡å¤§çš„ç­–ç•¥æ›´æ–°")
    
    print("\nâœ… ç®—æ³•ä¼˜åŠ¿:")
    advantages = [
        "å†…å­˜æ•ˆç‡ï¼šä¸éœ€è¦è®­ç»ƒå•ç‹¬çš„ä»·å€¼å‡½æ•°ç½‘ç»œ",
        "è®¡ç®—ç®€å•ï¼šç¾¤ç»„ç›¸å¯¹ä¼˜åŠ¿è®¡ç®—ç›´è§‚ä¸”é«˜æ•ˆ", 
        "è®­ç»ƒç¨³å®šï¼šè£å‰ªæœºåˆ¶ç¡®ä¿ç­–ç•¥æ›´æ–°çš„ç¨³å®šæ€§",
        "æ•ˆæœæ˜¾è‘—ï¼šåœ¨æ•°å­¦æ¨ç†ç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚"
    ]
    
    for advantage in advantages:
        print(f"   â€¢ {advantage}")


def simulate_grpo_training_step():
    """
    æ¨¡æ‹Ÿå•æ­¥GRPOè®­ç»ƒè¿‡ç¨‹
    """
    print("\n" + "="*80)
    print("GRPOè®­ç»ƒæ­¥éª¤æ¼”ç¤º - æ•°å­¦é—®é¢˜: 2 + 3 = ?")
    print("="*80)
    
    # è®¾ç½®å‚æ•°
    group_size = 8
    epsilon = 0.2
    prompt = "2 + 3 = ?"
    
    # 1. ç”Ÿæˆå“åº”ç¾¤ç»„
    print("\n1. ç”Ÿæˆå“åº”ç¾¤ç»„:")
    responses = [
        "è§£ç­”ï¼š2 + 3çš„ç­”æ¡ˆæ˜¯5",
        "è®¡ç®—ï¼š2 + 3 = 5", 
        "è®©æˆ‘ç®—ä¸€ä¸‹ï¼š2 + 3ç­‰äº5",
        "ç®€å•è®¡ç®—ï¼š2 + 3çš„ç»“æœæ˜¯5",
        "æ•°å­¦è®¡ç®—ï¼š2 + 3 = 6",  # é”™è¯¯ç­”æ¡ˆ
        "æˆ‘è§‰å¾—2 + 3ç­‰äº4",    # é”™è¯¯ç­”æ¡ˆ
        "æ ¹æ®è®¡ç®—ï¼š2 + 3 = 5",
        "ç­”æ¡ˆï¼š2 + 3æ˜¯5"
    ]
    
    for i, response in enumerate(responses):
        print(f"   å“åº”{i+1}: {response}")
    
    # 2. è®¡ç®—å¥–åŠ±
    print("\n2. è®¡ç®—å¥–åŠ±:")
    rewards = []
    
    for i, response in enumerate(responses):
        if "= 5" in response or "æ˜¯5" in response or "ç­‰äº5" in response:
            # æ­£ç¡®ç­”æ¡ˆï¼Œæ·»åŠ ä¸€äº›éšæœºæ€§
            reward = 1.0 + random.uniform(-0.1, 0.1)
        elif "= 6" in response or "æ˜¯6" in response or "ç­‰äº6" in response:
            # é”™è¯¯ç­”æ¡ˆ
            reward = -0.5 + random.uniform(-0.1, 0.1)
        elif "= 4" in response or "æ˜¯4" in response or "ç­‰äº4" in response:
            # é”™è¯¯ç­”æ¡ˆ
            reward = -0.3 + random.uniform(-0.1, 0.1)
        else:
            # å…¶ä»–æƒ…å†µ
            reward = 0.0 + random.uniform(-0.2, 0.2)
        
        rewards.append(reward)
        print(f"   å“åº”{i+1}: {reward:.3f}")
    
    # 3. è®¡ç®—ç¾¤ç»„ç›¸å¯¹ä¼˜åŠ¿
    print("\n3. è®¡ç®—ç¾¤ç»„ç›¸å¯¹ä¼˜åŠ¿:")
    mean_reward = sum(rewards) / len(rewards)
    advantages = [reward - mean_reward for reward in rewards]
    
    print(f"   ç¾¤ç»„å¹³å‡å¥–åŠ±: {mean_reward:.3f}")
    print("   å„å“åº”ä¼˜åŠ¿:")
    for i, advantage in enumerate(advantages):
        status = "æ­£ä¼˜åŠ¿" if advantage > 0 else "è´Ÿä¼˜åŠ¿"
        print(f"     å“åº”{i+1}: {advantage:+.3f} ({status})")
    
    # 4. è®¡ç®—é‡è¦æ€§æ¯”ç‡ï¼ˆæ¨¡æ‹Ÿï¼‰
    print("\n4. è®¡ç®—é‡è¦æ€§æ¯”ç‡:")
    ratios = []
    for advantage in advantages:
        # æ­£ä¼˜åŠ¿çš„å“åº”æœ‰æ›´é«˜çš„æ¯”ç‡
        if advantage > 0:
            ratio = random.uniform(1.0, 1.5)
        else:
            ratio = random.uniform(0.7, 1.0)
        ratios.append(ratio)
    
    for i, ratio in enumerate(ratios):
        print(f"   å“åº”{i+1}: {ratio:.3f}")
    
    # 5. è®¡ç®—GRPOè£å‰ªç›®æ ‡
    print("\n5. è®¡ç®—GRPOè£å‰ªç›®æ ‡:")
    objectives = []
    
    for i, (ratio, advantage) in enumerate(zip(ratios, advantages)):
        # è£å‰ªé‡è¦æ€§æ¯”ç‡
        clipped_ratio = max(1 - epsilon, min(1 + epsilon, ratio))
        
        # è®¡ç®—ä¸¤ä¸ªç›®æ ‡å€¼å¹¶å–æœ€å°å€¼
        obj1 = ratio * advantage
        obj2 = clipped_ratio * advantage
        objective = min(obj1, obj2)
        objectives.append(objective)
        
        print(f"   å“åº”{i+1}: ratio={ratio:.3f}, clipped={clipped_ratio:.3f}, "
              f"advantage={advantage:+.3f}, objective={objective:+.3f}")
    
    total_objective = sum(objectives) / len(objectives)
    print(f"\n   å¹³å‡ç›®æ ‡å€¼: {total_objective:.3f}")
    
    return {
        'mean_reward': mean_reward,
        'total_objective': total_objective,
        'positive_advantages': sum(1 for a in advantages if a > 0),
        'negative_advantages': sum(1 for a in advantages if a < 0)
    }


def simulate_multi_step_training():
    """
    æ¨¡æ‹Ÿå¤šæ­¥è®­ç»ƒè¿‡ç¨‹
    """
    print("\n" + "="*80)
    print("GRPOå¤šæ­¥è®­ç»ƒæ¨¡æ‹Ÿ")
    print("="*80)
    
    prompts = [
        "2 + 3 = ?",
        "1 + 4 = ?", 
        "3 + 2 = ?",
        "4 + 1 = ?",
        "0 + 5 = ?"
    ]
    
    training_stats = []
    
    for step in range(10):
        # éšæœºé€‰æ‹©ä¸€ä¸ªprompt
        prompt = random.choice(prompts)
        
        print(f"\nç¬¬{step+1}æ­¥è®­ç»ƒ - Prompt: {prompt}")
        
        # ç®€åŒ–çš„è®­ç»ƒæ­¥éª¤
        # ç”Ÿæˆæ¨¡æ‹Ÿå¥–åŠ±
        rewards = []
        for _ in range(8):  # group_size = 8
            if random.random() < 0.7:  # 70%æ¦‚ç‡æ­£ç¡®
                reward = 1.0 + random.uniform(-0.1, 0.1)
            else:
                reward = -0.5 + random.uniform(-0.2, 0.2)
            rewards.append(reward)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        mean_reward = sum(rewards) / len(rewards)
        advantages = [r - mean_reward for r in rewards]
        pos_advantages = sum(1 for a in advantages if a > 0)
        neg_advantages = sum(1 for a in advantages if a < 0)
        
        print(f"  å¹³å‡å¥–åŠ±: {mean_reward:.3f}")
        print(f"  æ­£ä¼˜åŠ¿å“åº”: {pos_advantages}/8")
        print(f"  è´Ÿä¼˜åŠ¿å“åº”: {neg_advantages}/8")
        
        training_stats.append({
            'step': step + 1,
            'mean_reward': mean_reward,
            'positive_ratio': pos_advantages / 8
        })
    
    # æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡
    print(f"\n{'='*60}")
    print("è®­ç»ƒç»Ÿè®¡æ€»ç»“:")
    print(f"{'='*60}")
    
    final_stats = training_stats[-5:]  # æœ€å5æ­¥
    avg_reward = sum(s['mean_reward'] for s in final_stats) / len(final_stats)
    avg_positive_ratio = sum(s['positive_ratio'] for s in final_stats) / len(final_stats)
    
    print(f"æœ€å5æ­¥å¹³å‡å¥–åŠ±: {avg_reward:.3f}")
    print(f"æœ€å5æ­¥æ­£ä¼˜åŠ¿æ¯”ä¾‹: {avg_positive_ratio:.3f}")
    
    if avg_positive_ratio > 0.6:
        print("âœ… è®­ç»ƒæ•ˆæœè‰¯å¥½ï¼æ­£ä¼˜åŠ¿å“åº”æ¯”ä¾‹è¾ƒé«˜")
    elif avg_positive_ratio > 0.4:
        print("âš ï¸  è®­ç»ƒæ•ˆæœä¸€èˆ¬ï¼Œéœ€è¦æ›´å¤šè¿­ä»£")
    else:
        print("âŒ è®­ç»ƒæ•ˆæœä¸ä½³ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´å‚æ•°")
    
    return training_stats


def compare_with_ppo():
    """
    ä¸PPOç®—æ³•çš„å¯¹æ¯”
    """
    print("\n" + "="*80)
    print("GRPO vs PPO ç®—æ³•å¯¹æ¯”")
    print("="*80)
    
    print("\nğŸ“Š ç®—æ³•å¯¹æ¯”è¡¨:")
    print("-" * 70)
    print(f"{'ç‰¹æ€§':<15} {'PPO':<25} {'GRPO':<25}")
    print("-" * 70)
    print(f"{'ä¼˜åŠ¿ä¼°è®¡':<15} {'éœ€è¦ä»·å€¼å‡½æ•° V(s)':<25} {'ä½¿ç”¨ç¾¤ç»„å‡å€¼ RÌ„':<25}")
    print(f"{'å†…å­˜ä½¿ç”¨':<15} {'é«˜ï¼ˆéœ€è¦ä»·å€¼ç½‘ç»œï¼‰':<25} {'ä½ï¼ˆæ— éœ€ä»·å€¼ç½‘ç»œï¼‰':<25}")
    print(f"{'è®¡ç®—å¤æ‚åº¦':<15} {'é«˜':<25} {'ä½':<25}")
    print(f"{'è®­ç»ƒç¨³å®šæ€§':<15} {'ç¨³å®š':<25} {'ç¨³å®š':<25}")
    print(f"{'é€‚ç”¨åœºæ™¯':<15} {'é€šç”¨RLä»»åŠ¡':<25} {'æ–‡æœ¬ç”Ÿæˆä»»åŠ¡':<25}")
    print("-" * 70)
    
    print("\nğŸ’¡ æ ¸å¿ƒåŒºåˆ«:")
    print("   PPO: éœ€è¦ä»·å€¼å‡½æ•°ä¼°è®¡ä¼˜åŠ¿ â†’ A = Q(s,a) - V(s)")
    print("   GRPO: ä½¿ç”¨ç¾¤ç»„å‡å€¼ä¼°è®¡ä¼˜åŠ¿ â†’ A = R - RÌ„")
    print("   ç»“æœ: GRPOå†…å­˜ä½¿ç”¨æ›´å°‘ï¼Œè®¡ç®—æ›´ç®€å•")


def show_applications():
    """
    å±•ç¤ºGRPOçš„åº”ç”¨åœºæ™¯
    """
    print("\n" + "="*80)
    print("GRPOç®—æ³•åº”ç”¨åœºæ™¯")
    print("="*80)
    
    print("\nğŸ¯ ä¸»è¦åº”ç”¨é¢†åŸŸ:")
    applications = [
        ("æ•°å­¦é—®é¢˜æ±‚è§£", "GSM8K, MATHç­‰æ•°å­¦æ¨ç†ä»»åŠ¡"),
        ("ä»£ç ç”Ÿæˆ", "ç¼–ç¨‹é—®é¢˜çš„è‡ªåŠ¨è§£ç­”"),
        ("é€»è¾‘æ¨ç†", "éœ€è¦å¤šæ­¥æ¨ç†çš„å¤æ‚é—®é¢˜"),
        ("ç²¾ç¡®ç­”æ¡ˆéªŒè¯", "æœ‰æ˜ç¡®æ­£ç¡®ç­”æ¡ˆçš„ä»»åŠ¡")
    ]
    
    for app, desc in applications:
        print(f"   â€¢ {app}: {desc}")
    
    print("\nğŸ“ˆ å®é™…æ•ˆæœ (DeepSeek R1):")
    results = [
        ("GSM8K", "ä»82.9%æå‡åˆ°88.2%"),
        ("MATH", "ä»46.8%æå‡åˆ°51.7%"),
        ("CMATH", "ä»84.6%æå‡åˆ°88.8%")
    ]
    
    for task, improvement in results:
        print(f"   â€¢ {task}: {improvement}")
    
    print("\nğŸ”¬ æŠ€æœ¯ä¼˜åŠ¿:")
    tech_advantages = [
        "ç›¸æ¯”PPOå‡å°‘çº¦50%çš„GPUå†…å­˜ä½¿ç”¨",
        "ç®—æ³•é€»è¾‘æ¸…æ™°ï¼Œæ˜“äºç†è§£å’Œå®ç°",
        "åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡æ€§èƒ½",
        "è®­ç»ƒè¿‡ç¨‹æ›´åŠ ç¨³å®š"
    ]
    
    for advantage in tech_advantages:
        print(f"   â€¢ {advantage}")


def main():
    """
    ä¸»å‡½æ•°ï¼šè¿è¡Œå®Œæ•´çš„GRPOç®—æ³•æ¼”ç¤º
    """
    # è®¾ç½®éšæœºç§å­ä»¥è·å¾—å¯é‡ç°çš„ç»“æœ
    random.seed(42)
    
    # 1. ç®—æ³•åŸç†è§£é‡Š
    demonstrate_grpo_algorithm()
    
    # 2. å•æ­¥è®­ç»ƒæ¼”ç¤º
    simulate_grpo_training_step()
    
    # 3. å¤šæ­¥è®­ç»ƒæ¨¡æ‹Ÿ
    simulate_multi_step_training()
    
    # 4. ä¸PPOå¯¹æ¯”
    compare_with_ppo()
    
    # 5. åº”ç”¨åœºæ™¯å±•ç¤º
    show_applications()
    
    # æ€»ç»“
    print(f"\n{'='*80}")
    print("ğŸ‰ GRPOç®—æ³•æ¼”ç¤ºå®Œæˆï¼")
    print("="*80)
    print("\nğŸ“ æ€»ç»“:")
    print("è¿™ä¸ªæ¼”ç¤ºå±•ç¤ºäº†GRPOçš„æ ¸å¿ƒæ€æƒ³ï¼š")
    print("1. ç¾¤ç»„ç”Ÿæˆå¤šä¸ªå“åº”")
    print("2. ä½¿ç”¨ç¾¤ç»„å‡å€¼ä½œä¸ºåŸºçº¿è®¡ç®—ç›¸å¯¹ä¼˜åŠ¿") 
    print("3. é€šè¿‡è£å‰ªç›®æ ‡å‡½æ•°ç¨³å®šè®­ç»ƒ")
    print("4. é¿å…è®­ç»ƒä»·å€¼å‡½æ•°ï¼Œæé«˜å†…å­˜æ•ˆç‡")
    
    print("\nğŸ”— å‚è€ƒæ–‡çŒ®:")
    print("â€¢ DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models")
    print("â€¢ DeepSeek R1: Incentivizing reasoning capability in llms via reinforcement learning")
    print("â€¢ Proximal Policy Optimization Algorithms")
    
    print(f"\n{'='*80}")


if __name__ == "__main__":
    main()